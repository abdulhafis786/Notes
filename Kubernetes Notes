https://kubernetes.io/docs/reference/kubectl/cheatsheet/ ---- %%% Cheat sheet %%%

Minikube is a single instance of Kubernetes cluster 

minikube version -- to get the Version of Minikube 

minikube start --wait=false -- to start the minikube 

minikube is a Single instance cluster. To INteract with it we need to have commands executed via kubectl.

kubectl cluster-info -- to get the info about the cluster.

kubectl get nodes -- To get the nodes runnning in the cluster.

The flow of Kubernetes is as Cluster -> Nodes -> Pods -> Containers. A node may be a physical or virtual Machine which could 
contain multiple pods and each pod may contain multiple container depending on the input given.

kubectl create deployment first-deployment --image=katacoda/docker-http-server --- This will create a first deployment on the cluster.

kubectl get pods -- to get the number of pods running.

kubectl expose deployment first-deployment --port=80 --type=NodePort - this is to expose the deployment whihc was done on the cluster.
Node port option is given to assign Dynamic port on the system.

kubectl get svc first-deployment -o go-template='{{range.spec.ports}}{{if .nodePort}}{{.nodePort}}{{"\n"}}{{end}}{{end}}' -- To get the port number of the Deployment done.

kubectl get svc first-deployment -- This will give the Details of the first-deployment done.Here first deployment is the name .

minikube addons enable dashboard --- To enable the addon for the dashboard. 

kubectl apply -f /opt/kubernetes-dashboard.yaml -- To apply the dashboard. the contenst of the yaml file is below,

apiVersion: v1
kind: Service
metadata:
  labels:
    app: kubernetes-dashboard
  name: kubernetes-dashboard-katacoda
  namespace: kube-system
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 9090
    nodePort: 30000
  selector:
    app: kubernetes-dashboard
  type: NodePort

kubectl get pods -n kube-system  -w -- to show the pods associated with the kube-system which is the dashboard.

If to run multiple cluster, we start with Kubeadm

kubeadm version -o short --- To get the version of kubeadm

kubeadm init --token=102952.1a7dd4cc8d1f4cc5 --kubernetes-version $(kubeadm version -o short) -- to initialize the kubeadm.
In Production the Token option is not needed as kubeadm will generate one for us.

The Container Network Interface (CNI) defines how the different nodes and their workloads should communicate
More information can be obtained at https://kubernetes.io/docs/concepts/cluster-administration/addons/

kubectl apply -f /opt/weave-kube --- here weave-kube is applied as container network interface.

kubeadm token list -- to list the tokens availabe in the master. This is used so that other members could join the master.

kubeadm join --discovery-token-unsafe-skip-ca-verification --token=102952.1a7dd4cc8d1f4cc5 172.17.0.47:6443 ->
This is for the node to join the master using the token generated from the master.

The --discovery-token-unsafe-skip-ca-verification tag is used to bypass the Discovery Token verification. As this token is generated dynamically, we couldn't include it within the steps. 

kubectl run http --image=katakoda/docker-http-server:latest --replicas=1 -- This will run the image http on the kubernetes cluster.
here running the container is nothing but deploying the container.

kubectl get deployments -- to get the number of deployments running on the server. here deployment is nothing but the images running.

kubectl describe deployment http -- gives a detailed information about the deployment of the image on the cluster.

kubectl expose deployment http --external-ip="172.17.0.19" --port=8000 --target-port=80 -- Here we are exposing the deployment
to an external IP and a port to be accessible from the outside network.

kubectl run httpexposed --image=katacoda/docker-http-server:latest --replicas=1 --port=80 --hostport=8001 -- this creates a deployment and 
exposes the port to the external network

kubectl get svc -- to get the list of services listed and exposed. if the above command is used to create and expose a deployment,
it wont show in this command output.

docker ps | grep httpexposed --- this will show that the deployment created and exposed in a single command (line 79)

Running the above command you'll notice the ports are exposed on the Pause, not the http container itself. 
The Pause container is responsible for defining the network for the Pod. 
Other containers in the pod share the same network namespace. 
This improves network performance and allow multiple containers to communicate over the same network interface.

eg:
$ docker ps | grep httpexposed
387fe2365c34        katacoda/docker-http-server   "/app"                   About a minute ago   Up About a minute                          k8s_httpexposed_httpexposed-569df5d86-pf8n4_default_3bab7c5e-b0a1-41fb-8b61-220a25588ed6_0
bb3ae1928210        k8s.gcr.io/pause:3.1          "/pause"                 About a minute ago   Up About a minute   0.0.0.0:8001->80/tcp   k8s_POD_httpexposed-569df5d86-pf8n4_default_3bab7c5e-b0a1-41fb-8b61-220a25588ed6_0

kubectl scale --replicas=3 deployment http -- to scale the replicas created for the deployment. Replicas are nothing but the images

kubectl create -f deployment.yaml -- to create a deployment using an Yaml file. here all the necessary information is given in the
file and the deployment is run.

******REF******
Example Yaml file is as below:
apiVersion: extensions/v1beta1
kind: Deployment #refers to the kind of activity done by K8. There are may types like Pod,service etc
metadata:
  name: webapp1 #Name of the container 
spec:
  replicas: 1 #Number of replicas needed
  template:
    metadata:
      labels:
        app: webapp1 #Here label is mentioned cos multiple images of the same label can be clustered together
    spec:
      containers:
      - name: webapp1
        image: katacoda/docker-http-server:latest #Image from which the container could be created
        ports:
        - containerPort: 80 # exposing port 
        
Exaple of a service Yaml file:
apiVersion: v1
kind: Service
metadata:
  name: webapp1-svc #name of the service
  labels:
    app: webapp1
spec:
  type: NodePort #Type of service 
  ports:
  - port: 80  #Container port 
    nodePort: 30080 #Exposed port 
  selector:
    app: webapp1 #App for which the service is to be enabled.
    
******REF******
kubectl apply -f deployment.yaml --- when there is a change in any value defined under deployment.yaml file apply command must be given.

kubectl cluster-info ---  to get the info on kubernetes master and the DNS 

kubectl get rc -- command to get the replication controller 

******REF******
The replication controller defines how many instances should be running, the Docker Image to use, and a name to identify the service. 
Additional options can be utilized for configuration and discovery.If a container were to go down, the replication controller would restart it on an active node

A Kubernetes service is a named load balancer that proxies traffic to one or more containers. 
The proxy works even if the containers are on different nodes.

Services proxy communicate within the cluster and rarely expose ports to an outside interface.

When you launch a service it looks like you cannot connect using curl or netcat unless you start it as part of Kubernetes. 
The recommended approach is to have a LoadBalancer service to handle external communications.

******REF******
******REF******

apiVersion: v1
kind: ReplicationController
metadata:
  name: redis-master
  labels:
    name: redis-master
spec:
  replicas: 1
  selector:
    name: redis-master
  template:
    metadata:
      labels:
        name: redis-master
    spec:
      containers:
      - name: master
        image: redis:3.0.7-alpine
        ports:
        - containerPort: 6379
 
 apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    name: redis-master
spec:
  ports:
    # the port that this service should serve on
  - port: 6379
    targetPort: 6379
  selector:
    name: redis-master
    
 apiVersion: v1
kind: ReplicationController
metadata:
  name: redis-slave
  labels:
    name: redis-slave
spec:
  replicas: 2
  selector:
    name: redis-slave
  template:
    metadata:
      labels:
        name: redis-slave
    spec:
      containers:
      - name: worker
        image: gcr.io/google_samples/gb-redisslave:v1
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below.
          # value: env
        ports:
        - containerPort: 6379
        
  apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    name: redis-slave
spec:
  ports:
    # the port that this service should serve on
  - port: 6379
  selector:
    name: redis-slave
    
 apiVersion: v1
kind: ReplicationController
metadata:
  name: frontend
  labels:
    name: frontend
spec:
  replicas: 3
  selector:
    name: frontend
  template:
    metadata:
      labels:
        name: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google_samples/gb-frontend:v3
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below.
          # value: env
        ports:
        - containerPort: 80

apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    name: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  type: NodePort
  ports:
    # the port that this service should serve on
    - port: 80
      nodePort: 30080
  selector:
    name: frontend

******REF******
******REF******

Cluster IP is the default approach when creating a Kubernetes Service. 
The service is allocated an internal IP that other components can use to access the pods.

By having a single IP address it enables the service to be load balanced across multiple Pods

apiVersion: v1
kind: Service
metadata:
  name: webapp1-clusterip-svc
  labels:
    app: webapp1-clusterip
spec:
  ports:
  - port: 80
  selector:
    app: webapp1-clusterip
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: webapp1-clusterip-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: webapp1-clusterip
    spec:
      containers:
      - name: webapp1-clusterip-pod
        image: katacoda/docker-http-server:latest
        ports:
        - containerPort: 80
---
******REF******

kubectl describe svc/webapp1-clusterip-svc -- to describe a particular service.

kubectl get pods -n kube-system -- to get the detail of a specific pod.

******REF******

Target ports allows us to separate the port the service is available on from the port the application is listening on. 
TargetPort is the Port which the application is configured to listen on. 
Port is how the application will be accessed from the outside.

apiVersion: v1
kind: Service
metadata:
  name: webapp1-clusterip-targetport-svc
  labels:
    app: webapp1-clusterip-targetport
spec:
  ports:
  - port: 8080
    targetPort: 80
  selector:
    app: webapp1-clusterip-targetport
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: webapp1-clusterip-targetport-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: webapp1-clusterip-targetport
    spec:
      containers:
      - name: webapp1-clusterip-targetport-pod
        image: katacoda/docker-http-server:latest
        ports:
        - containerPort: 80
---

While TargetPort and ClusterIP make it available to inside the cluster, the NodePort exposes the service on each Node’s IP via the defined static port. 
No matter which Node within the cluster is accessed, the service will be reachable based on the port number defined.

apiVersion: v1
kind: Service
metadata:
  name: webapp1-nodeport-svc
  labels:
    app: webapp1-nodeport
spec:
  type: NodePort
  ports:
  - port: 80
    nodePort: 30080
  selector:
    app: webapp1-nodeport
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: webapp1-nodeport-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: webapp1-nodeport
    spec:
      containers:
      - name: webapp1-nodeport-pod
        image: katacoda/docker-http-server:latest
        ports:
        - containerPort: 80
---

The service can now be reached via the Node's IP address on the NodePort defined.

curl 172.17.0.25:30080 -- Here the IP given is the machine's IP.

Another approach to making a service available outside of the cluster is via External IP addresses.

apiVersion: v1
kind: Service
metadata:
  name: webapp1-externalip-svc
  labels:
    app: webapp1-externalip
spec:
  ports:
  - port: 80
  externalIPs:
  - 172.17.0.25
  selector:
    app: webapp1-externalip
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: webapp1-externalip-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: webapp1-externalip
    spec:
      containers:
      - name: webapp1-externalip-pod
        image: katacoda/docker-http-server:latest
        ports:
        - containerPort: 80
---

apiVersion: v1
kind: Service
metadata:
  name: webapp1-loadbalancer-svc
  labels:
    app: webapp1-loadbalancer
spec:
  type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: webapp1-loadbalancer
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: webapp1-loadbalancer-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: webapp1-loadbalancer
    spec:
      containers:
      - name: webapp1-loadbalancer-pod
        image: katacoda/docker-http-server:latest
        ports:
        - containerPort: 80
---

******REF******

Kubernetes have advanced networking capabilities that allow Pods and Services to communicate inside the cluster's network. 

An Ingress enables inbound connections to the cluster, allowing external traffic to reach the correct Pod.

Ingress enables externally-reachable urls, load balance traffic, terminate SSL, offer name based virtual hosting for a Kubernetes cluster.

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: webapp1
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: webapp1
    spec:
      containers:
      - name: webapp1
        image: katacoda/docker-http-server:latest
        ports:
        - containerPort: 80
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: webapp2
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: webapp2
    spec:
      containers:
      - name: webapp2
        image: katacoda/docker-http-server:latest
        ports:
        - containerPort: 80
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: webapp3
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: webapp3
    spec:
      containers:
      - name: webapp3
        image: katacoda/docker-http-server:latest
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: webapp1-svc
  labels:
    app: webapp1
spec:
  ports:
  - port: 80
  selector:
    app: webapp1
---
apiVersion: v1
kind: Service
metadata:
  name: webapp2-svc
  labels:
    app: webapp2
spec:
  ports:
  - port: 80
  selector:
    app: webapp2
---
apiVersion: v1
kind: Service
metadata:
  name: webapp3-svc
  labels:
    app: webapp3
spec:
  ports:
  - port: 80
  selector:
    app: webapp3
    
  apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: webapp-ingress
spec:
  rules:
  - host: my.kubernetes.example
    http:
      paths:
      - path: /webapp1
        backend:
          serviceName: webapp1-svc
          servicePort: 80
      - path: /webapp2
        backend:
          serviceName: webapp2-svc
          servicePort: 80
      - backend:
          serviceName: webapp3-svc
          servicePort: 80
          
  ******REF******
  
  kubectl get ing ---- to get the ingress rules.Ingress rules are an object type with Kubernetes. 
  The rules can be based on a request host (domain), or the path of the request, or a combination of both.
  
  kubectl get pods --selector="name=bad-frontend" --- to get the info of the particular node using selector.
  
  ******REF******
  Creating persistant stores
  
  apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-0001
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteOnce
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    server: 172.17.0.78
    path: /exports/data-0001
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-0002
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    server: 172.17.0.78
    path: /exports/data-0002
   
  ******REF******
  kubectl get pv --- To get the Persistance volumes in Kubernetes. They are the data volumes in which the application data could be stored.
  
  ******REF******
  To claim persistance we need to have persistance volume claim
  kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: claim-mysql
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: claim-http
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
      
  ******REF******
  
  kubectl get pvc --- to find the list of persistant vlume claim. The value defined in claim will take from any of the 
  persistance volume created.
  
  ******REF******
  
  apiVersion: v1
kind: Pod
metadata:
  name: mysql
  labels:
    name: mysql
spec:
  containers:
  - name: mysql
    image: openshift/mysql-55-centos7
    env:
      - name: MYSQL_ROOT_PASSWORD
        value: yourpassword
      - name: MYSQL_USER
        value: wp_user
      - name: MYSQL_PASSWORD
        value: wp_pass
      - name: MYSQL_DATABASE
        value: wp_db
    ports:
      - containerPort: 3306
        name: mysql
    volumeMounts:
      - name: mysql-persistent-storage
        mountPath: /var/lib/mysql/data
  volumes:
    - name: mysql-persistent-storage
      persistentVolumeClaim:
        claimName: claim-mysql
apiVersion: v1
kind: Pod
metadata:
  name: www
  labels:
    name: www
spec:
  containers:
  - name: www
    image: nginx:alpine
    ports:
      - containerPort: 80
        name: www
    volumeMounts:
      - name: www-persistent-storage
        mountPath: /usr/share/nginx/html
  volumes:
    - name: www-persistent-storage
      persistentVolumeClaim:
        claimName: claim-http
        
   The above one is to create a Pod which claims the persistance volume already present while initializing the Pod.
   
   ******REF******
   
   kubectl create -f pod-mysql.yaml -- to create a Pod from the yaml file.
   
   kubectl delete pod www -- to delete a POD created.
   
   ******REF*******
   
kind: PersistentVolume
apiVersion: v1
metadata:
  name: mongo-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: "/mnt/data/mongo"
 
 
 kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: mongo-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
      
 apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2
kind: Deployment
metadata:
  name: mongo
spec:
  selector:
    matchLabels:
      app: mongo
      role: master
      tier: backend
  replicas: 1
  template:
    metadata:
      labels:
        app: mongo
        role: master
        tier: backend
    spec:
      containers:
        - name: mongo
          image: mongo
          ports:
            - containerPort: 27017
          volumeMounts:
            - name: data
              mountPath: /data/db
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: mongo-pv-claim
  
  apiVersion: v1
kind: Service
metadata:
  name: mongo
  labels:
    app: mongo
    role: master
    tier: backend
spec:
  ports:
    - port: 27017
      targetPort: 27017
  # Replace with the IP of your minikube node / master node if you want external access
  # externalIPs:
  #   - xxx.zzz.yyy.qqq
  selector:
    app: mongo
    role: master
    tier: backend
 
 apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2
kind: Deployment
metadata:
  name: node
spec:
  selector:
    matchLabels:
      app: node
      tier: backend
  replicas: 9
  template:
    metadata:
      labels:
        app: node
        tier: backend
    spec:
      containers:
        - name: node
          image: adnanrahic/boilerplate-api:latest
          ports:
            - containerPort: 3000
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
 
 apiVersion: v1
kind: Service
metadata:
  name: node
  labels:
    app: node
    tier: backend
spec:
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: 3000
  # Replace with the IP of your minikube node / master node
  externalIPs:
    - xxx.yyy.zzz.qqq
  selector:
    app: node
    tier: backend
  
  ******REF******
 
 kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta1/aio/deploy/recommended.yaml --- This file is needed
 to get the access for the Kubernetes console.
 
 kubectl proxy -- this has to be run after executing the above.
 
 Once the above command is done, use the URL to access the console,
 
 http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
 
There are two ways to access the dashboard one is to create a service account and another way is to use a kubeconfig file.
For token generation, we could follow the content as below,

https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md


 
   
   
   
